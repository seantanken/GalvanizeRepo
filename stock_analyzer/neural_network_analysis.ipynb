{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "intimate-magnet",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.metrics import *\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, LSTM\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import *\n",
    "\n",
    "import numpy.random as rand\n",
    "import scipy.stats as stats \n",
    "from keras.wrappers.scikit_learn import KerasClassifier, KerasRegressor \n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "medical-charleston",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import date, timedelta, time, datetime\n",
    "import time\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dramatic-customs",
   "metadata": {},
   "source": [
    "Use root mean squared log error\n",
    "\n",
    "Pipeline?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fleet-arctic",
   "metadata": {},
   "source": [
    "train = 3 or 4 weeks, test = 1 day / week"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "representative-height",
   "metadata": {},
   "source": [
    "## Train/Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "particular-plant",
   "metadata": {},
   "source": [
    "Creating train/test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "further-maker",
   "metadata": {},
   "source": [
    "train_test_split(y, shuffle=False) instead of manually selecting train/test data?\n",
    "\n",
    "or df2 = datasX.iloc[:, :72]\n",
    "   df2 = datasX.iloc[:, 72:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "forbidden-strain",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "immediate-allocation",
   "metadata": {},
   "outputs": [],
   "source": [
    "week_start = '2021-05-31'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "mounted-river",
   "metadata": {},
   "outputs": [],
   "source": [
    "cptv_week_csv = f'AMZN_cptv_{week_start}.csv'\n",
    "cptv_week_df = pd.read_csv(f'C:/Users/seant/stock_analyzer/weekly_data/{cptv_week_csv}', header=0, index_col=0)\n",
    "train = train.append(cptv_week_df, ignore_index=True, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "younger-passing",
   "metadata": {},
   "outputs": [],
   "source": [
    "cptv_week_csv = f'AMZN_cptv_{week_start}.csv'\n",
    "test = pd.read_csv(f'C:/Users/seant/stock_analyzer/weekly_data/{cptv_week_csv}', header=0, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "grand-peter",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv = f'AMZN_cptv_train.csv'\n",
    "train.to_csv(path_or_buf=f'C:/Users/seant/stock_analyzer/weekly_data/{train_csv}')\n",
    "test_csv = f'AMZN_cptv_test.csv'\n",
    "test.to_csv(path_or_buf=f'C:/Users/seant/stock_analyzer/weekly_data/{test_csv}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "identical-imperial",
   "metadata": {},
   "source": [
    "Massaging data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "removed-perth",
   "metadata": {},
   "outputs": [],
   "source": [
    "def roll(df, column_name_lst, minute_period=200):\n",
    "    rolled_df = pd.DataFrame()\n",
    "    for column_name in column_name_lst:\n",
    "        series = df.loc[:, column_name]\n",
    "        rolled_df[column_name] = series.rolling(minute_period, center=True).mean()\n",
    "    return rolled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "civilian-mauritius",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decompose(df, column_name_lst, minute_period=390):\n",
    "    df_copy = df.copy()\n",
    "    seasonal_resid_df = pd.DataFrame()\n",
    "    trend_df = pd.DataFrame()\n",
    "    for column_name in column_name_lst:\n",
    "        for_decompose = df_copy[[column_name]].dropna()\n",
    "        decomposed = seasonal_decompose(for_decompose, model='additive', period=minute_period)\n",
    "        trend_df[column_name] = decomposed.trend.dropna()\n",
    "        seasonal_resid_df[f'{column_name}_seasonal'] = decomposed.seasonal\n",
    "        seasonal_resid_df[f'{column_name}_resid'] = decomposed.resid\n",
    "    #add a column of constants?\n",
    "    return trend_df, seasonal_resid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cultural-beaver",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(trend_df, minute_period=390, num_periods = 5):\n",
    "    num_prediction_periods = minute_period * num_periods\n",
    "    last_period_cut = trend_df.shape(0) - num_prediction_periods #want to use 5 periods of data to predict next period of close prices\n",
    "    X = trend_df.iloc[:, :Last_period_cut] #cuts last period off for features to predict from\n",
    "    y = trend_df.iloc[0, num_prediction_periods:] #cuts first column and period off for targets for prediction\n",
    "    #how to use 1 week of data to predict 1 day of prices? extrapolate trend out 1 period?\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifteen-stephen",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recompose(df, seasonal_resid_df, column_name_lst, minute_period=390):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "royal-generator",
   "metadata": {},
   "outputs": [],
   "source": [
    "minute_rolling_avg = 200\n",
    "\n",
    "train_rolled = pd.DataFrame()\n",
    "train_close_series = train.loc[:, 'Close']\n",
    "train_rolled = train_rolled.assign(rolling_close = train_close_series.rolling(minute_rolling_avg, center=True).mean())\n",
    "train_volume_series = train.loc[:, 'Volume']\n",
    "train_rolled = train_rolled.assign(rolling_volume = train_volume_series.rolling(minute_rolling_avg, center=True).mean())\n",
    "\n",
    "test_rolled = pd.DataFrame()\n",
    "test_close_series = test.loc[:, 'Close']\n",
    "test_rolled = test_rolled.assign(rolling_close = test_close_series.rolling(minute_rolling_avg, center=True).mean())\n",
    "test_volume_series = test.loc[:, 'Volume']\n",
    "test_rolled = test_rolled.assign(rolling_volume = test_volume_series.rolling(minute_rolling_avg, center=True).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "rapid-awareness",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Close      3197.335587\n",
       "Volume    11301.540000\n",
       "Name: 500, dtype: float64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_rolled = roll(train, ['Close', 'Volume'])\n",
    "train_rolled.iloc[500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "textile-hydrogen",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7729, 5)\n",
      "(1931, 3)\n"
     ]
    }
   ],
   "source": [
    "cptv_copy = cptv_week_df.copy()\n",
    "cptv_roll_close_for_decompose = cptv_copy[['rolling_close']].dropna()\n",
    "cptv_roll_vol_for_decompose = cptv_copy[['rolling_volume']].dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optimum-sheffield",
   "metadata": {},
   "source": [
    "X_train = combine all weeks into one dataframe, decompose into closing(trend_1 + trend_2), volume(trend), +1 column, add additional day as prediction happens to predict next day\n",
    "\n",
    "y_train = closing(trend_1 + trend_2)? target is next minute price? min, max of next day?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specialized-brand",
   "metadata": {},
   "source": [
    "neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "purple-chapel",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(units=10, return_sequences=True))\n",
    "model.add(Dropout(rate=0.2))\n",
    "model.add(LSTM(units=10, return_sequences=True))\n",
    "model.add(Dropout(rate=0.2))\n",
    "model.add(LSTM(units=10, return_sequences=True))\n",
    "model.add(Dropout(rate=0.2))\n",
    "#model.add(Dense(5, activation='relu'))\n",
    "model.add(Dense(5))\n",
    "model.compile(loss='root_mean_squared_error', optimizer='SGD', metrics=keras.metrics.Accuracy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "municipal-gospel",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 300\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dried-patrol",
   "metadata": {},
   "outputs": [],
   "source": [
    "results=model.evaluate(X_test, y_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "graduate-directory",
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildNN(X, y, activation_function, metric, optimizer, hidden_layers=[7], batch_size=32, epochs=5):\n",
    "    skf = StratifiedKFold()\n",
    "    skf.get_n_splits(X, y)\n",
    "    sumofsquares = np.zeros([13])\n",
    "    sumofweights = np.zeros([13])\n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "        #X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.33, random_state=42)\n",
    "        model = keras.models.Sequential()\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        for i, layer in enumerate(hidden_layers):\n",
    "            if i == 0:\n",
    "                model.add(keras.layers.Dense(units=layer, input_dim=13, activation=activation_function))\n",
    "            else:\n",
    "                model.add(keras.layers.Dense(units=layer, activation=activation_function))\n",
    "        model.add(keras.layers.Dense(units=1, activation=activation_function))\n",
    "        model.compile(loss='mean_squared_error', optimizer=optimizer, metrics=keras.metrics.BinaryAccuracy()) #'sgd 'keras.metrics.BinaryAccuracy()\n",
    "        model.summary()\n",
    "        model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size)\n",
    "        print(\"Test Results:\")\n",
    "        results = model.evaluate(X_test, y_test, batch_size=batch_size)\n",
    "        for i, layer_size in enumerate(hidden_layers):\n",
    "            for index in range(layer_size):\n",
    "                print(f\"hidden_layers[{i}] = index=[{index}] {model.weights[0].numpy()[:,index]}\")\n",
    "                sumofsquares += model.weights[0].numpy()[:,index]**2\n",
    "                sumofweights += model.weights[0].numpy()[:,index]\n",
    "                #print(f\"model.weights len = {len(model.weights)}\")\n",
    "    print(\"absvalues=\")\n",
    "    print(absvalues)\n",
    "    return sumofsquares,sumofweights"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
